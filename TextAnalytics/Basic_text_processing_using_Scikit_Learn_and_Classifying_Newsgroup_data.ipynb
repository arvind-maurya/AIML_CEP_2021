{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic text processing using Scikit Learn and Classifying Newsgroup data",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balamurugan-palaniappan-CEP/AIML_CEP_2021/blob/main/TextAnalytics/Basic_text_processing_using_Scikit_Learn_and_Classifying_Newsgroup_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXAYgkuEoocv",
        "outputId": "d59a30c0-bf80-45c7-e81f-9f2d1b6d1064"
      },
      "source": [
        "#import package from scikit-learn containing 20newsgroups data \n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "full_train_data_labels = fetch_20newsgroups(subset = 'train')\n",
        "\n",
        "#print the labels in the data\n",
        "#print(type(full_train_data_labels.target_names))\n",
        "print(list(set(full_train_data_labels.target_names)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comp.windows.x', 'sci.space', 'sci.med', 'talk.politics.mideast', 'alt.atheism', 'comp.sys.mac.hardware', 'comp.sys.ibm.pc.hardware', 'talk.politics.guns', 'misc.forsale', 'comp.graphics', 'talk.politics.misc', 'rec.autos', 'sci.electronics', 'comp.os.ms-windows.misc', 'rec.motorcycles', 'talk.religion.misc', 'soc.religion.christian', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJt0OMZHpx2L",
        "outputId": "73f44f96-55ce-40e1-b634-3a278cbf612a"
      },
      "source": [
        "categories_list = ['comp.graphics', 'sci.med']\n",
        "\n",
        "train_data_labels = fetch_20newsgroups(subset='train',categories=categories_list, shuffle=True)\n",
        "print(list(set(train_data_labels.target_names)))\n",
        "\n",
        "#print a few documents and corresponding labels \n",
        "num_documents = 3\n",
        "for i in range(num_documents):\n",
        "  print('Document:')\n",
        "  print(train_data_labels.data[i])\n",
        "  print('label:',train_data_labels.target[i])\n",
        "  print('##########################################################')\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comp.graphics', 'sci.med']\n",
            "Document:\n",
            "From: zyeh@caspian.usc.edu (zhenghao yeh)\n",
            "Subject: Re: Need polygon splitting algo...\n",
            "Organization: University of Southern California, Los Angeles, CA\n",
            "Lines: 25\n",
            "Distribution: world\n",
            "NNTP-Posting-Host: caspian.usc.edu\n",
            "Keywords: polygons, splitting, clipping\n",
            "\n",
            "\n",
            "In article <1qvq4b$r4t@wampyr.cc.uow.edu.au>, g9134255@wampyr.cc.uow.edu.au (Coronado Emmanuel Abad) writes:\n",
            "|> \n",
            "|> The idea is to clip one polygon using another polygon (not\n",
            "|> necessarily rectangular) as a window.  My problem then is in\n",
            "|> finding out all the new vertices of the resulting \"subpolygons\"\n",
            "|> from the first one.  Is this simply a matter of extending the\n",
            "|> usual algorithm whereby each of the edges of one polygon is checked\n",
            "|> against another polygon???  Is there a simpler way??\n",
            "|> \n",
            "|> Comments welcome.\n",
            "|> \n",
            "|> Noel.\n",
            "\n",
            "\tIt depends on what kind of the polygons. \n",
            "\tConvex - simple, concave - trouble, concave with loop(s)\n",
            "\tinside - big trouble.\n",
            "\n",
            "\tOf cause, you can use the box test to avoid checking\n",
            "\teach edges. According to my experience, there is not\n",
            "\ta simple way to go. The headache stuff is to deal with\n",
            "\tthe special cases, for example, the overlapped lines.\n",
            "\n",
            "\tYeh\n",
            "\tUSC\n",
            "\n",
            "label: 0\n",
            "##########################################################\n",
            "Document:\n",
            "From: jim.zisfein@factory.com (Jim Zisfein) \n",
            "Subject: Need advice with doctor-patient relationship problem\n",
            "Distribution: world\n",
            "Organization: Invention Factory's BBS - New York City, NY - 212-274-8298v.32bis\n",
            "Reply-To: jim.zisfein@factory.com (Jim Zisfein) \n",
            "Lines: 13\n",
            "\n",
            "ML> From: libman@hsc.usc.edu (Marlena Libman)\n",
            "ML> I need advice with a situation which occurred between me and a physican\n",
            "ML> which upset me.\n",
            "\n",
            "ML> My questions: (1) Should I continue to have this doctor manage my care?\n",
            "\n",
            "That's easy:  No.  You wouldn't take your computer into a repair\n",
            "shop where they were rude to you, even if they were competent in\n",
            "their business.  Why would you take your own body into a \"repair\n",
            "shop\" where the \"repairman\" has such a bad attitude?\n",
            "---\n",
            " . SLMR 2.1 . E-mail: jim.zisfein@factory.com (Jim Zisfein)\n",
            "                                              \n",
            "\n",
            "label: 1\n",
            "##########################################################\n",
            "Document:\n",
            "From: stefan@lis.e-technik.tu-muenchen.de (Stefan Eckart)\n",
            "Subject: dmpeg10.zip info: Another DOS MPEG decoder/player posted\n",
            "Keywords: MPEG, DOS\n",
            "Reply-To: stefan@lis.e-technik.tu-muenchen.de\n",
            "Organization: Technische Universitaet Muenchen, Germany\n",
            "Lines: 74\n",
            "\n",
            "\n",
            "I have posted a DOS MPEG decoder/player to alt.binaries.pictures.utilities.\n",
            "\n",
            "Here is a short description and some technical information, taken from the\n",
            "accompanying documentation:\n",
            "\n",
            "\n",
            "                              DMPEG V1.0\n",
            "\n",
            "                       Public Domain MPEG decoder\n",
            "\n",
            "                           by Stefan Eckart\n",
            "\n",
            "\n",
            "0. Features\n",
            "===========\n",
            "\n",
            "DMPEG/DMPLAY is another MPEG decoder/player for the PC:\n",
            "\n",
            "\n",
            " - decodes (nearly) the full MPEG video standard\n",
            "   (I,P,B frames, frame size up to at least 352x240 supported)\n",
            "\n",
            " - saves decoded sequence in 8 or 24bit raw file for later display\n",
            "\n",
            " - optional on-screen display during decoding (requires VGA)\n",
            "\n",
            " - several dithering options: ordered dither, Floyd-Steinberg, grayscale\n",
            "\n",
            " - color-space selection\n",
            "\n",
            " - runs under DOS, 640KB RAM, no MS-Windows required\n",
            "\n",
            " - very compact (small code / small data models, 16 bit arithmetic)\n",
            "\n",
            " - real time display of the raw file by a separate player for\n",
            "   VGA and many Super-VGAs\n",
            "\n",
            "...\n",
            "\n",
            "4. Technical information\n",
            "========================\n",
            "\n",
            "The player is a rather straightforward implementation of the MPEG spec [1].\n",
            "The IDCT is based on the Chen-Wang 13 multiplication algorithm [2]\n",
            "(not quite the optimum, I know). Blocks with not more than eight non-zero\n",
            "coefficients use a non-separated direct multiply-accumulate 2D-IDCT\n",
            "(sounds great, doesn't it?), which turned out to be faster than a 'fast'\n",
            "algorithm in this (quite common) case. Dithering is pretty standard. Main\n",
            "difference to the Berkeley decoder (except for the fewer number of supported\n",
            "algorithms) is the use of 256 instead of 128 colors, the (default) option to\n",
            "use a restricted color-space and the implementation of a color saturation\n",
            "dominant ordered dither. This leads to a significantly superior quality of\n",
            "the dithered image (I claim, judge yourself).\n",
            "\n",
            "Restricted color-space means that the U and V components are clipped to\n",
            "+/-0.25 (instead of +/-0.5) and the display color-space points are distributed\n",
            "over this restricted space. Since the distance between color-space points\n",
            "is thus reduced by a factor of two, the color resolution is doubled at the\n",
            "expense of not being able to represent fully saturated colors.\n",
            "\n",
            "Saturation dominant ordered dither is a method by which a color, lying\n",
            "somewhere between the points of the display color space, is approximated\n",
            "by primarily alternating between two points of constant hue instead of\n",
            "constant saturation. This yields subjectivly better quality due to the\n",
            "lower sensitivity of the human viewing system to saturation changes than\n",
            "to hue changes (the same reasoning as used by the PAL TV standard to improve\n",
            "on NTSC). The improvement is particularly visible in dark brown or redish\n",
            "areas.\n",
            "\n",
            "...\n",
            "\n",
            "--\n",
            "Stefan Eckart, stefan@lis.e-technik.tu-muenchen.de\n",
            "\n",
            "label: 0\n",
            "##########################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4czAV06zwQG"
      },
      "source": [
        "$\\textbf{About converting text features to numerical attributes}$ $\\textit{ (From scikit-learn website)}$: \n",
        "\n",
        "Scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
        "\n",
        "\n",
        "1.   $\\textbf{tokenizing}$ strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
        "2.   $\\textbf{counting}$ the occurrences of tokens in each document.\n",
        "3.   $\\textbf{normalizing and weighting}$ with diminishing importance for tokens that occur in majority of samples / documents.\n",
        "\n",
        "\n",
        "In this scheme, features and samples are defined as follows:\n",
        "\n",
        "1.   Each individual token occurrence frequency (normalized or not) is treated as a $\\textbf{feature}$\n",
        "2.   The vector of all the token frequencies for a given document is considered a $\\textbf{multivariate sample}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FbqDiA3tGo-"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5azGOfiE1LPH"
      },
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56MEdBlJ1NK_"
      },
      "source": [
        "corpus = [\n",
        "...     'This is the first sample document.',\n",
        "...     'This is another document called the second sample document.',\n",
        "...     'And the third sample document.',\n",
        "...     'Is this the first sample document?',\n",
        "... ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA1i-RAN1V5r",
        "outputId": "365586a4-1496-4371-9b3b-2ef180877512"
      },
      "source": [
        "X = vectorizer.fit_transform(corpus)\n",
        "print(type(X))\n",
        "#print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJsxe6Hm1axl",
        "outputId": "76abfcae-761c-457e-8351-62e8a9a447e6"
      },
      "source": [
        "#unique words obtained from the corpus. Note: All words have lower case letters\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'another', 'called', 'document', 'first', 'is', 'sample', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGLzzsxFTQ11"
      },
      "source": [
        "$\\textbf{Note:}$ CountVectorizer by default tokenizes string by extracting words of at least 2 letters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'This is another document called the second sample document.',"
      ],
      "metadata": {
        "id": "iyh6PBoARUIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKj1Fpvu1gQX",
        "outputId": "20e1bf87-41b6-494e-de6a-4bcf213d669a"
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 1 1 1 1 0 1 0 1]\n",
            " [0 1 1 2 0 1 1 1 1 0 1]\n",
            " [1 0 0 1 0 0 1 0 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQQJ9rnr1kN6",
        "outputId": "f8976e2f-eac8-49d1-d05b-7e6e8ef312ef"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 10, 'is': 5, 'the': 8, 'first': 4, 'sample': 6, 'document': 3, 'another': 1, 'called': 2, 'second': 7, 'and': 0, 'third': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj3zdmeucF3r",
        "outputId": "a3350aa4-b506-4e42-990d-8cf1c9dcc892"
      },
      "source": [
        "#to get the feature identifier of a particular word \n",
        "print(vectorizer.vocabulary_.get('called'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krKRpVAhcR08",
        "outputId": "0a997f2e-fd07-488f-87b0-85a5c5372f1e"
      },
      "source": [
        "#we can use the vectorizer object created earlier to encode new sentences \n",
        "#However if the new sentence contains new words, they would not be added to the vocabulary \n",
        "vectorizer.transform(['completely new sentence.']).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWurTeQfdERp",
        "outputId": "bb6e4626-7291-4c60-af94-5cb3fc67b7c2"
      },
      "source": [
        "vectorizer.transform(['another new sample document.']).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYp49rmvdMsN"
      },
      "source": [
        "#if we want to rebuild the vectorizer we must include the new documents in corpus\n",
        "#and then reconstruct again \n",
        "#Consider this as an exercise "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3jBrfCMeb7R",
        "outputId": "2d583e50-be11-45a2-8d8c-3d0cd267a605"
      },
      "source": [
        "#recall: in our corpus the first document is 'This is the first sample document.', \n",
        "#and the last document is 'Is this the first sample document?'\n",
        "print(X.toarray()[0])\n",
        "print(X.toarray()[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 1 1 1 1 0 1 0 1]\n",
            "[0 0 0 1 1 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoiFis-eerUC"
      },
      "source": [
        "#To account for the word order we can use bigrams and n-grams containing multiple words as features \n",
        "#we will build another vectorizer using bigram features now \n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(1,2)) #consider both unigrams and bigrams "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJykrqdXfI7Z",
        "outputId": "6c2a2732-e4e4-4e65-cd78-84deff9cac3a"
      },
      "source": [
        "X_unigram_bigram = bigram_vectorizer.fit_transform(corpus)\n",
        "print(type(X_unigram_bigram))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYdumY5-e_a_",
        "outputId": "9a9c1831-2659-4e43-be1d-ab7f7b70f683"
      },
      "source": [
        "\n",
        "print(bigram_vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'and the', 'another', 'another document', 'called', 'called the', 'document', 'document called', 'first', 'first sample', 'is', 'is another', 'is the', 'is this', 'sample', 'sample document', 'second', 'second sample', 'the', 'the first', 'the second', 'the third', 'third', 'third sample', 'this', 'this is', 'this the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTdNneDEfsBv",
        "outputId": "2e278415-c77d-4d97-c565-4363e4e04ade"
      },
      "source": [
        "print(len(bigram_vectorizer.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmLOTTw3gpMj",
        "outputId": "613857f6-fcb9-4351-a878-2516a9711db3"
      },
      "source": [
        "bigram_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'and the': 1,\n",
              " 'another': 2,\n",
              " 'another document': 3,\n",
              " 'called': 4,\n",
              " 'called the': 5,\n",
              " 'document': 6,\n",
              " 'document called': 7,\n",
              " 'first': 8,\n",
              " 'first sample': 9,\n",
              " 'is': 10,\n",
              " 'is another': 11,\n",
              " 'is the': 12,\n",
              " 'is this': 13,\n",
              " 'sample': 14,\n",
              " 'sample document': 15,\n",
              " 'second': 16,\n",
              " 'second sample': 17,\n",
              " 'the': 18,\n",
              " 'the first': 19,\n",
              " 'the second': 20,\n",
              " 'the third': 21,\n",
              " 'third': 22,\n",
              " 'third sample': 23,\n",
              " 'this': 24,\n",
              " 'this is': 25,\n",
              " 'this the': 26}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4XA2fBDfGn6",
        "outputId": "ec808577-8354-4c7f-eaa2-625e9c13d3e2"
      },
      "source": [
        "X_unigram_bigram.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
              "        0, 0, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
              "        0, 0, 1, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
              "        1, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
              "        0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0i0BUtJfcF7",
        "outputId": "b265153e-7616-4d10-aee0-fc74b536979b"
      },
      "source": [
        "#Now let us compare the features of first and last document\n",
        "print(X_unigram_bigram.toarray()[0])\n",
        "print(X_unigram_bigram.toarray()[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0]\n",
            "[0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1LtExJsgjKQ"
      },
      "source": [
        "#Exercise: Try to build a vectorizer with unigram, bigram and trigram features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pQP2Rnthpk6"
      },
      "source": [
        "$\\textbf{TF-IDF}:$ The term frequency-inverse document frequency (TF-IDF) re-weighting is useful to reduce the dominance of count based features used for very frequent words which might not have much significance.\n",
        "\n",
        "$\\textbf{Term Frequency}:$ $\\texttt{tf(t,d)}$ denotes the frequency of term $\\texttt{t}$ in document $\\texttt{d}$. \n",
        "\n",
        "$\\textbf{Inverse Document Frequency}:$ $\\texttt{idf(t)}$ is proportional to the inverse of the number of documents containing the term $\\texttt{t}$. The formula to compute $\\texttt{idf(t)}$ is: \n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\texttt{idf(t)} = \\log \\frac{1+n}{1+\\texttt{df(t)}} + 1\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "where $n$ denotes the number of documents in the corpus and $\\texttt{df(t)}$ denotes the number of documents in the corpus containing term $\\texttt{t}$. The addition with $1$ is useful for avoiding numerical issues. \n",
        "\n",
        "Thus $\\textbf{TF-IDF}$ is calculated as: $\\texttt{tf-idf(t,d)}=\\texttt{tf(t,d)} \\times \\texttt{idf(t)}$.  \n",
        "\n",
        "After composing the vector for each document, the vectors are normalized so that they are of unit length in terms of Euclidean norm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_EnLGixh7jX"
      },
      "source": [
        "corpus_2 = [ 'The boy came to the school in the morning by the bus.', \n",
        "             'He took his book and wrote in the book using his pencil.',\n",
        "             'He played the game using the ball.',\n",
        "             'He took his bag, left the school by the bus and reached his home.',           \n",
        "             ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1u8zjv5i1Cw"
      },
      "source": [
        "#a simple count based vectorizer gives too much significance to words like \"the\", \"his\"\n",
        "vectorizer_corpus_2 = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "573ft_d-jB4z"
      },
      "source": [
        "X_corpus_2 = vectorizer_corpus_2.fit_transform(corpus_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1sqgqIcjJhF",
        "outputId": "a0dd4eeb-d717-45e1-837b-54d71bde8b6d"
      },
      "source": [
        "print(vectorizer_corpus_2.get_feature_names()) \n",
        "print('num features:',len(vectorizer_corpus_2.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'bag', 'ball', 'book', 'boy', 'bus', 'by', 'came', 'game', 'he', 'his', 'home', 'in', 'left', 'morning', 'pencil', 'played', 'reached', 'school', 'the', 'to', 'took', 'using', 'wrote']\n",
            "num features: 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBNcj57pjRB5",
        "outputId": "9eae7715-15c2-45d1-8bf8-ccdc59708465"
      },
      "source": [
        "print(vectorizer_corpus_2.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 19, 'boy': 4, 'came': 7, 'to': 20, 'school': 18, 'in': 12, 'morning': 14, 'by': 6, 'bus': 5, 'he': 9, 'took': 21, 'his': 10, 'book': 3, 'and': 0, 'wrote': 23, 'using': 22, 'pencil': 15, 'played': 16, 'game': 8, 'ball': 2, 'bag': 1, 'left': 13, 'reached': 17, 'home': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V42VGEm2jWE2",
        "outputId": "6e038abf-abb9-4fdc-87ab-9b3db66f9606"
      },
      "source": [
        "X_corpus_2.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 4, 1, 0,\n",
              "        0, 0],\n",
              "       [1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
              "        1, 1],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0,\n",
              "        1, 0],\n",
              "       [1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1, 1, 2, 0, 1,\n",
              "        0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKQdokEdj42P"
      },
      "source": [
        "#Let us transform the features using a tf-idf based weight scheme\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs6z_U6emiVs"
      },
      "source": [
        "tfidf_X_corpus_2 = tfidf_transformer.fit_transform(X_corpus_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3PXW9Ytml3d",
        "outputId": "0e5748c3-4760-4faa-9b23-817552b417f5"
      },
      "source": [
        "print(tfidf_X_corpus_2.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.30367981 0.23942465\n",
            "  0.23942465 0.30367981 0.         0.         0.         0.\n",
            "  0.23942465 0.         0.30367981 0.         0.         0.\n",
            "  0.23942465 0.63389088 0.30367981 0.         0.         0.        ]\n",
            " [0.23096381 0.         0.         0.58589662 0.         0.\n",
            "  0.         0.         0.         0.18698511 0.46192763 0.\n",
            "  0.23096381 0.         0.         0.29294831 0.         0.\n",
            "  0.         0.15287258 0.         0.23096381 0.23096381 0.29294831]\n",
            " [0.         0.         0.44201611 0.         0.         0.\n",
            "  0.         0.         0.44201611 0.28213316 0.         0.\n",
            "  0.         0.         0.         0.         0.44201611 0.\n",
            "  0.         0.46132469 0.         0.         0.34849058 0.        ]\n",
            " [0.23673759 0.30027162 0.         0.         0.         0.23673759\n",
            "  0.23673759 0.         0.         0.19165948 0.47347519 0.30027162\n",
            "  0.         0.30027162 0.         0.         0.         0.30027162\n",
            "  0.23673759 0.31338837 0.         0.23673759 0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry8pf5l_oBDF",
        "outputId": "78617931-fc34-40ec-9795-2c7dc292f727"
      },
      "source": [
        "#check if each row has unit Euclidean norm\n",
        "import numpy as np\n",
        "X1 = tfidf_X_corpus_2.toarray()\n",
        "for i in range(len(X1)):\n",
        "  print(np.linalg.norm(X1[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n",
            "0.9999999999999999\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZKF0M2C1Xu8"
      },
      "source": [
        "$\\textbf{Getting back to 20 Newsgroups data:}$ We now consider the text processing of the data corresponding to two categories $\\texttt{comp.graphics}$, $\\texttt{sci.med}$.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeP8nmBr3ALY",
        "outputId": "69e0ccaf-b642-4b18-d225-018e38ca0e25"
      },
      "source": [
        "print('num documents:', len(train_data_labels.data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num documents: 1178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH7Z3JtO2HNS"
      },
      "source": [
        "data_20newsgroups_vectorizer = CountVectorizer() #we consider only unigrams here "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syxxlmPi2Qks"
      },
      "source": [
        "X_train_20newsgroups = data_20newsgroups_vectorizer.fit_transform(train_data_labels.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvMU1-r02nBQ",
        "outputId": "787ddd2e-2d2c-4dde-f048-ff6539e169cc"
      },
      "source": [
        "num_features = len(data_20newsgroups_vectorizer.get_feature_names())\n",
        "print('num features:', num_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num features: 24614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLIEgY_J3jTw",
        "outputId": "99dfe59f-e4e1-4c0a-8656-3ccbea0fe43f"
      },
      "source": [
        "print(X_train_20newsgroups.toarray().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1178, 24614)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S875SRdL8jcF",
        "outputId": "79d0dc22-c9ba-4a33-ee60-1e1eedfbf698"
      },
      "source": [
        "num_samples = X_train_20newsgroups.toarray().shape[0]\n",
        "print('num samples:', num_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num samples: 1178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY7r39dO2dih",
        "outputId": "bed7623e-9faa-4cd9-e2f6-bc901c3b0ede"
      },
      "source": [
        "y_train_20newsgroups = train_data_labels.target\n",
        "print(y_train_20newsgroups.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1178,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwemoRsgzP56"
      },
      "source": [
        "import torch\n",
        "class SimpleFeedforwardNet(torch.nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            super(SimpleFeedforwardNet, self).__init__()\n",
        "            self.input_size = input_size\n",
        "            self.hidden_size  = hidden_size\n",
        "            self.output_size = output_size\n",
        "            #first fully connected layer connecting input to hidden layer\n",
        "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "            self.relu = torch.nn.ReLU()\n",
        "            #second fully connected layer connecting hidden layer to output layer\n",
        "            self.fc2 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "            self.sigmoid = torch.nn.Sigmoid()        \n",
        "        #Define how forward pass is to be performed\n",
        "        def forward(self, x):\n",
        "            hidden = self.fc1(x)\n",
        "            relu = self.relu(hidden)\n",
        "            output = self.fc2(relu)\n",
        "            output = self.sigmoid(output)\n",
        "            return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQNuakOg1FVl"
      },
      "source": [
        "model = SimpleFeedforwardNet(num_features, 128,1) \n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQJWjUmd2GAb",
        "outputId": "c7d65c8b-af51-4a3f-9347-a89d7d337a94"
      },
      "source": [
        "#set model to train mode\n",
        "model.train()\n",
        "\n",
        "num_epochs = 400\n",
        "\n",
        "loss_epochs = []\n",
        "x_train = torch.FloatTensor(X_train_20newsgroups.toarray())\n",
        "y_train = torch.FloatTensor(y_train_20newsgroups).view(num_samples, 1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  #set optimizer gradients to zero     \n",
        "  optimizer.zero_grad()    \n",
        "  #Get prediction output from model on train data using forward pass \n",
        "  y_pred = model(x_train)    \n",
        "  #compute loss between predicted labels and actual labels\n",
        "  loss = criterion(y_pred, y_train)\n",
        "  #keeping track of losses\n",
        "  loss_epochs.append(loss) \n",
        "  #It is useful to compute the performance of model on train data in terms of accuracy\n",
        "  y_pred_cpu = y_pred.squeeze().detach().cpu().numpy()\n",
        "  #print(type(y_pred_cpu))\n",
        "  y_pred_cpu = np.multiply(y_pred_cpu>0.5,1)\n",
        "  #print(y_pred_cpu)\n",
        "  #print(y_train_20newsgroups)\n",
        "  train_accuracy = 100.*np.sum(1-np.abs(y_pred_cpu-y_train_20newsgroups))/len(y_train_20newsgroups)\n",
        "  print('Epoch {}: train loss: {} train accuracy: {}'.format(epoch, loss.item(), train_accuracy))   \n",
        "  # Backward pass to propagate loss (or error) gradients \n",
        "  loss.backward() #this is automatically taken care of by Pytorch\n",
        "  optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: train loss: 0.6939827799797058 train accuracy: 49.745331069609506\n",
            "Epoch 1: train loss: 0.6904149651527405 train accuracy: 50.254668930390494\n",
            "Epoch 2: train loss: 0.6876462697982788 train accuracy: 50.33955857385399\n",
            "Epoch 3: train loss: 0.685201108455658 train accuracy: 50.33955857385399\n",
            "Epoch 4: train loss: 0.6829267144203186 train accuracy: 50.42444821731749\n",
            "Epoch 5: train loss: 0.6807618737220764 train accuracy: 50.42444821731749\n",
            "Epoch 6: train loss: 0.6786473393440247 train accuracy: 50.42444821731749\n",
            "Epoch 7: train loss: 0.6766060590744019 train accuracy: 50.50933786078099\n",
            "Epoch 8: train loss: 0.6746106743812561 train accuracy: 50.59422750424448\n",
            "Epoch 9: train loss: 0.6726210713386536 train accuracy: 50.59422750424448\n",
            "Epoch 10: train loss: 0.6706365346908569 train accuracy: 50.84889643463497\n",
            "Epoch 11: train loss: 0.6686738729476929 train accuracy: 51.01867572156197\n",
            "Epoch 12: train loss: 0.6667467951774597 train accuracy: 51.188455008488965\n",
            "Epoch 13: train loss: 0.6648620367050171 train accuracy: 51.188455008488965\n",
            "Epoch 14: train loss: 0.663005530834198 train accuracy: 51.188455008488965\n",
            "Epoch 15: train loss: 0.6611636877059937 train accuracy: 51.27334465195246\n",
            "Epoch 16: train loss: 0.6593186259269714 train accuracy: 51.44312393887946\n",
            "Epoch 17: train loss: 0.6574754118919373 train accuracy: 51.52801358234296\n",
            "Epoch 18: train loss: 0.6556032299995422 train accuracy: 51.86757215619694\n",
            "Epoch 19: train loss: 0.6536726355552673 train accuracy: 52.12224108658744\n",
            "Epoch 20: train loss: 0.651719331741333 train accuracy: 52.29202037351443\n",
            "Epoch 21: train loss: 0.6497932076454163 train accuracy: 52.29202037351443\n",
            "Epoch 22: train loss: 0.6478762626647949 train accuracy: 52.54668930390493\n",
            "Epoch 23: train loss: 0.6460059285163879 train accuracy: 53.05602716468591\n",
            "Epoch 24: train loss: 0.644152820110321 train accuracy: 53.735144312393885\n",
            "Epoch 25: train loss: 0.6423230767250061 train accuracy: 54.41426146010187\n",
            "Epoch 26: train loss: 0.6405114531517029 train accuracy: 54.66893039049236\n",
            "Epoch 27: train loss: 0.638708770275116 train accuracy: 55.00848896434635\n",
            "Epoch 28: train loss: 0.6369317770004272 train accuracy: 55.34804753820034\n",
            "Epoch 29: train loss: 0.6351721882820129 train accuracy: 55.94227504244482\n",
            "Epoch 30: train loss: 0.6334220767021179 train accuracy: 56.28183361629881\n",
            "Epoch 31: train loss: 0.6316782236099243 train accuracy: 57.130730050933785\n",
            "Epoch 32: train loss: 0.629943311214447 train accuracy: 58.064516129032256\n",
            "Epoch 33: train loss: 0.6282126903533936 train accuracy: 58.65874363327674\n",
            "Epoch 34: train loss: 0.6264848113059998 train accuracy: 59.592529711375214\n",
            "Epoch 35: train loss: 0.6247523427009583 train accuracy: 60.44142614601019\n",
            "Epoch 36: train loss: 0.6230300068855286 train accuracy: 61.37521222410866\n",
            "Epoch 37: train loss: 0.6213184595108032 train accuracy: 62.13921901528013\n",
            "Epoch 38: train loss: 0.6196131110191345 train accuracy: 62.81833616298812\n",
            "Epoch 39: train loss: 0.617914080619812 train accuracy: 63.58234295415959\n",
            "Epoch 40: train loss: 0.6162239909172058 train accuracy: 64.09168081494057\n",
            "Epoch 41: train loss: 0.614534854888916 train accuracy: 64.68590831918506\n",
            "Epoch 42: train loss: 0.6128414273262024 train accuracy: 65.53480475382004\n",
            "Epoch 43: train loss: 0.6111516356468201 train accuracy: 66.12903225806451\n",
            "Epoch 44: train loss: 0.6094653606414795 train accuracy: 67.14770797962649\n",
            "Epoch 45: train loss: 0.6077819466590881 train accuracy: 68.25127334465195\n",
            "Epoch 46: train loss: 0.6061094403266907 train accuracy: 68.76061120543294\n",
            "Epoch 47: train loss: 0.6044440865516663 train accuracy: 69.60950764006792\n",
            "Epoch 48: train loss: 0.6027774810791016 train accuracy: 71.05263157894737\n",
            "Epoch 49: train loss: 0.6011134386062622 train accuracy: 71.64685908319186\n",
            "Epoch 50: train loss: 0.59945148229599 train accuracy: 72.41086587436332\n",
            "Epoch 51: train loss: 0.5977908968925476 train accuracy: 73.00509337860781\n",
            "Epoch 52: train loss: 0.5961321592330933 train accuracy: 74.02376910016977\n",
            "Epoch 53: train loss: 0.5944750905036926 train accuracy: 74.87266553480475\n",
            "Epoch 54: train loss: 0.5928208231925964 train accuracy: 75.63667232597624\n",
            "Epoch 55: train loss: 0.5911683440208435 train accuracy: 76.4855687606112\n",
            "Epoch 56: train loss: 0.5895137786865234 train accuracy: 76.99490662139219\n",
            "Epoch 57: train loss: 0.5878612399101257 train accuracy: 77.41935483870968\n",
            "Epoch 58: train loss: 0.5862111449241638 train accuracy: 78.18336162988115\n",
            "Epoch 59: train loss: 0.5845652222633362 train accuracy: 78.86247877758913\n",
            "Epoch 60: train loss: 0.5829213857650757 train accuracy: 79.79626485568761\n",
            "Epoch 61: train loss: 0.5812774300575256 train accuracy: 80.1358234295416\n",
            "Epoch 62: train loss: 0.5796307325363159 train accuracy: 80.56027164685908\n",
            "Epoch 63: train loss: 0.5779858827590942 train accuracy: 80.81494057724957\n",
            "Epoch 64: train loss: 0.5763410329818726 train accuracy: 81.32427843803056\n",
            "Epoch 65: train loss: 0.5746944546699524 train accuracy: 81.66383701188455\n",
            "Epoch 66: train loss: 0.5730462074279785 train accuracy: 82.25806451612904\n",
            "Epoch 67: train loss: 0.5713978409767151 train accuracy: 82.42784380305602\n",
            "Epoch 68: train loss: 0.5697503089904785 train accuracy: 83.02207130730051\n",
            "Epoch 69: train loss: 0.568102240562439 train accuracy: 83.1918505942275\n",
            "Epoch 70: train loss: 0.5664544701576233 train accuracy: 83.70118845500849\n",
            "Epoch 71: train loss: 0.5648072361946106 train accuracy: 83.95585738539899\n",
            "Epoch 72: train loss: 0.5631614327430725 train accuracy: 84.21052631578948\n",
            "Epoch 73: train loss: 0.5615164637565613 train accuracy: 84.88964346349745\n",
            "Epoch 74: train loss: 0.5598706603050232 train accuracy: 84.88964346349745\n",
            "Epoch 75: train loss: 0.5582233667373657 train accuracy: 85.48387096774194\n",
            "Epoch 76: train loss: 0.5565773844718933 train accuracy: 85.56876061120543\n",
            "Epoch 77: train loss: 0.5549315214157104 train accuracy: 85.56876061120543\n",
            "Epoch 78: train loss: 0.5532862544059753 train accuracy: 85.65365025466893\n",
            "Epoch 79: train loss: 0.5516421794891357 train accuracy: 85.90831918505943\n",
            "Epoch 80: train loss: 0.550000786781311 train accuracy: 86.41765704584041\n",
            "Epoch 81: train loss: 0.5483598709106445 train accuracy: 86.41765704584041\n",
            "Epoch 82: train loss: 0.546719491481781 train accuracy: 86.5874363327674\n",
            "Epoch 83: train loss: 0.5450798273086548 train accuracy: 86.92699490662139\n",
            "Epoch 84: train loss: 0.5434409976005554 train accuracy: 87.26655348047538\n",
            "Epoch 85: train loss: 0.5418037176132202 train accuracy: 87.18166383701188\n",
            "Epoch 86: train loss: 0.5401691198348999 train accuracy: 87.26655348047538\n",
            "Epoch 87: train loss: 0.5385364890098572 train accuracy: 87.60611205432937\n",
            "Epoch 88: train loss: 0.53690505027771 train accuracy: 87.60611205432937\n",
            "Epoch 89: train loss: 0.5352742671966553 train accuracy: 87.69100169779287\n",
            "Epoch 90: train loss: 0.5336453914642334 train accuracy: 87.69100169779287\n",
            "Epoch 91: train loss: 0.5320180654525757 train accuracy: 87.94567062818336\n",
            "Epoch 92: train loss: 0.5303917527198792 train accuracy: 88.11544991511036\n",
            "Epoch 93: train loss: 0.5287681818008423 train accuracy: 88.11544991511036\n",
            "Epoch 94: train loss: 0.5271461606025696 train accuracy: 88.11544991511036\n",
            "Epoch 95: train loss: 0.525526762008667 train accuracy: 88.11544991511036\n",
            "Epoch 96: train loss: 0.5239095091819763 train accuracy: 88.11544991511036\n",
            "Epoch 97: train loss: 0.5222946405410767 train accuracy: 88.11544991511036\n",
            "Epoch 98: train loss: 0.520682692527771 train accuracy: 88.11544991511036\n",
            "Epoch 99: train loss: 0.519072949886322 train accuracy: 88.37011884550085\n",
            "Epoch 100: train loss: 0.5174648761749268 train accuracy: 88.62478777589133\n",
            "Epoch 101: train loss: 0.515859067440033 train accuracy: 88.53989813242785\n",
            "Epoch 102: train loss: 0.5142554640769958 train accuracy: 88.70967741935483\n",
            "Epoch 103: train loss: 0.5126540660858154 train accuracy: 88.87945670628183\n",
            "Epoch 104: train loss: 0.5110551118850708 train accuracy: 89.13412563667232\n",
            "Epoch 105: train loss: 0.5094582438468933 train accuracy: 89.21901528013582\n",
            "Epoch 106: train loss: 0.5078637599945068 train accuracy: 89.13412563667232\n",
            "Epoch 107: train loss: 0.5062714219093323 train accuracy: 89.04923599320882\n",
            "Epoch 108: train loss: 0.5046811103820801 train accuracy: 89.13412563667232\n",
            "Epoch 109: train loss: 0.5030931830406189 train accuracy: 89.21901528013582\n",
            "Epoch 110: train loss: 0.501507580280304 train accuracy: 89.13412563667232\n",
            "Epoch 111: train loss: 0.4999251961708069 train accuracy: 89.21901528013582\n",
            "Epoch 112: train loss: 0.49834609031677246 train accuracy: 89.30390492359932\n",
            "Epoch 113: train loss: 0.4967704713344574 train accuracy: 89.47368421052632\n",
            "Epoch 114: train loss: 0.49519795179367065 train accuracy: 89.47368421052632\n",
            "Epoch 115: train loss: 0.49362847208976746 train accuracy: 89.55857385398981\n",
            "Epoch 116: train loss: 0.49206164479255676 train accuracy: 89.81324278438031\n",
            "Epoch 117: train loss: 0.4904974699020386 train accuracy: 89.9830220713073\n",
            "Epoch 118: train loss: 0.4889357388019562 train accuracy: 90.0679117147708\n",
            "Epoch 119: train loss: 0.48737695813179016 train accuracy: 90.0679117147708\n",
            "Epoch 120: train loss: 0.4858212471008301 train accuracy: 90.0679117147708\n",
            "Epoch 121: train loss: 0.4842686951160431 train accuracy: 90.0679117147708\n",
            "Epoch 122: train loss: 0.48271939158439636 train accuracy: 90.1528013582343\n",
            "Epoch 123: train loss: 0.4811730980873108 train accuracy: 90.0679117147708\n",
            "Epoch 124: train loss: 0.47962984442710876 train accuracy: 90.1528013582343\n",
            "Epoch 125: train loss: 0.478089839220047 train accuracy: 90.2376910016978\n",
            "Epoch 126: train loss: 0.4765525460243225 train accuracy: 90.2376910016978\n",
            "Epoch 127: train loss: 0.47501838207244873 train accuracy: 90.3225806451613\n",
            "Epoch 128: train loss: 0.4734869599342346 train accuracy: 90.3225806451613\n",
            "Epoch 129: train loss: 0.47195905447006226 train accuracy: 90.2376910016978\n",
            "Epoch 130: train loss: 0.47043392062187195 train accuracy: 90.3225806451613\n",
            "Epoch 131: train loss: 0.4689120054244995 train accuracy: 90.49235993208829\n",
            "Epoch 132: train loss: 0.46739324927330017 train accuracy: 90.49235993208829\n",
            "Epoch 133: train loss: 0.46587854623794556 train accuracy: 90.4074702886248\n",
            "Epoch 134: train loss: 0.46436765789985657 train accuracy: 90.49235993208829\n",
            "Epoch 135: train loss: 0.46286067366600037 train accuracy: 90.57724957555179\n",
            "Epoch 136: train loss: 0.461357444524765 train accuracy: 90.66213921901527\n",
            "Epoch 137: train loss: 0.4598577320575714 train accuracy: 90.66213921901527\n",
            "Epoch 138: train loss: 0.4583609700202942 train accuracy: 90.66213921901527\n",
            "Epoch 139: train loss: 0.4568677842617035 train accuracy: 90.83191850594227\n",
            "Epoch 140: train loss: 0.4553789496421814 train accuracy: 90.66213921901527\n",
            "Epoch 141: train loss: 0.45389363169670105 train accuracy: 90.66213921901527\n",
            "Epoch 142: train loss: 0.4524117410182953 train accuracy: 90.66213921901527\n",
            "Epoch 143: train loss: 0.4509340524673462 train accuracy: 90.57724957555179\n",
            "Epoch 144: train loss: 0.44946008920669556 train accuracy: 90.49235993208829\n",
            "Epoch 145: train loss: 0.4479902684688568 train accuracy: 90.3225806451613\n",
            "Epoch 146: train loss: 0.4465245008468628 train accuracy: 90.49235993208829\n",
            "Epoch 147: train loss: 0.4450629651546478 train accuracy: 90.66213921901527\n",
            "Epoch 148: train loss: 0.4436049163341522 train accuracy: 90.66213921901527\n",
            "Epoch 149: train loss: 0.44215065240859985 train accuracy: 90.66213921901527\n",
            "Epoch 150: train loss: 0.440700888633728 train accuracy: 90.74702886247877\n",
            "Epoch 151: train loss: 0.43925613164901733 train accuracy: 90.74702886247877\n",
            "Epoch 152: train loss: 0.4378155767917633 train accuracy: 90.74702886247877\n",
            "Epoch 153: train loss: 0.43637949228286743 train accuracy: 90.66213921901527\n",
            "Epoch 154: train loss: 0.4349476099014282 train accuracy: 90.74702886247877\n",
            "Epoch 155: train loss: 0.43351995944976807 train accuracy: 90.83191850594227\n",
            "Epoch 156: train loss: 0.43209660053253174 train accuracy: 90.83191850594227\n",
            "Epoch 157: train loss: 0.4306781589984894 train accuracy: 90.83191850594227\n",
            "Epoch 158: train loss: 0.42926472425460815 train accuracy: 90.74702886247877\n",
            "Epoch 159: train loss: 0.42785605788230896 train accuracy: 90.74702886247877\n",
            "Epoch 160: train loss: 0.42645198106765747 train accuracy: 90.74702886247877\n",
            "Epoch 161: train loss: 0.4250524044036865 train accuracy: 90.66213921901527\n",
            "Epoch 162: train loss: 0.42365771532058716 train accuracy: 90.83191850594227\n",
            "Epoch 163: train loss: 0.42226728796958923 train accuracy: 91.08658743633276\n",
            "Epoch 164: train loss: 0.42088136076927185 train accuracy: 91.08658743633276\n",
            "Epoch 165: train loss: 0.41949954628944397 train accuracy: 91.08658743633276\n",
            "Epoch 166: train loss: 0.4181225597858429 train accuracy: 91.17147707979626\n",
            "Epoch 167: train loss: 0.41675037145614624 train accuracy: 91.17147707979626\n",
            "Epoch 168: train loss: 0.4153830111026764 train accuracy: 91.25636672325976\n",
            "Epoch 169: train loss: 0.4140205681324005 train accuracy: 91.25636672325976\n",
            "Epoch 170: train loss: 0.41266295313835144 train accuracy: 91.25636672325976\n",
            "Epoch 171: train loss: 0.4113103747367859 train accuracy: 91.34125636672326\n",
            "Epoch 172: train loss: 0.40996256470680237 train accuracy: 91.34125636672326\n",
            "Epoch 173: train loss: 0.40861907601356506 train accuracy: 91.34125636672326\n",
            "Epoch 174: train loss: 0.40728050470352173 train accuracy: 91.42614601018676\n",
            "Epoch 175: train loss: 0.4059467017650604 train accuracy: 91.42614601018676\n",
            "Epoch 176: train loss: 0.4046180248260498 train accuracy: 91.42614601018676\n",
            "Epoch 177: train loss: 0.40329447388648987 train accuracy: 91.51103565365025\n",
            "Epoch 178: train loss: 0.4019763469696045 train accuracy: 91.51103565365025\n",
            "Epoch 179: train loss: 0.4006634056568146 train accuracy: 91.51103565365025\n",
            "Epoch 180: train loss: 0.3993557393550873 train accuracy: 91.51103565365025\n",
            "Epoch 181: train loss: 0.3980533480644226 train accuracy: 91.59592529711375\n",
            "Epoch 182: train loss: 0.3967561721801758 train accuracy: 91.68081494057725\n",
            "Epoch 183: train loss: 0.3954642415046692 train accuracy: 91.68081494057725\n",
            "Epoch 184: train loss: 0.3941774368286133 train accuracy: 91.59592529711375\n",
            "Epoch 185: train loss: 0.3928956687450409 train accuracy: 91.68081494057725\n",
            "Epoch 186: train loss: 0.3916190266609192 train accuracy: 91.68081494057725\n",
            "Epoch 187: train loss: 0.3903474807739258 train accuracy: 91.68081494057725\n",
            "Epoch 188: train loss: 0.38908106088638306 train accuracy: 91.76570458404075\n",
            "Epoch 189: train loss: 0.38781994581222534 train accuracy: 91.76570458404075\n",
            "Epoch 190: train loss: 0.38656389713287354 train accuracy: 91.76570458404075\n",
            "Epoch 191: train loss: 0.38531261682510376 train accuracy: 91.85059422750425\n",
            "Epoch 192: train loss: 0.3840661942958832 train accuracy: 91.93548387096774\n",
            "Epoch 193: train loss: 0.38282492756843567 train accuracy: 91.93548387096774\n",
            "Epoch 194: train loss: 0.3815889060497284 train accuracy: 91.93548387096774\n",
            "Epoch 195: train loss: 0.3803580403327942 train accuracy: 92.02037351443124\n",
            "Epoch 196: train loss: 0.37913206219673157 train accuracy: 92.02037351443124\n",
            "Epoch 197: train loss: 0.37791115045547485 train accuracy: 92.02037351443124\n",
            "Epoch 198: train loss: 0.3766952455043793 train accuracy: 92.10526315789474\n",
            "Epoch 199: train loss: 0.37548425793647766 train accuracy: 92.10526315789474\n",
            "Epoch 200: train loss: 0.37427836656570435 train accuracy: 92.10526315789474\n",
            "Epoch 201: train loss: 0.3730774223804474 train accuracy: 92.10526315789474\n",
            "Epoch 202: train loss: 0.37188154458999634 train accuracy: 92.10526315789474\n",
            "Epoch 203: train loss: 0.37069079279899597 train accuracy: 92.10526315789474\n",
            "Epoch 204: train loss: 0.36950528621673584 train accuracy: 92.10526315789474\n",
            "Epoch 205: train loss: 0.36832481622695923 train accuracy: 92.19015280135824\n",
            "Epoch 206: train loss: 0.36714938282966614 train accuracy: 92.19015280135824\n",
            "Epoch 207: train loss: 0.36597898602485657 train accuracy: 92.27504244482174\n",
            "Epoch 208: train loss: 0.36481353640556335 train accuracy: 92.27504244482174\n",
            "Epoch 209: train loss: 0.36365294456481934 train accuracy: 92.27504244482174\n",
            "Epoch 210: train loss: 0.36249732971191406 train accuracy: 92.27504244482174\n",
            "Epoch 211: train loss: 0.36134687066078186 train accuracy: 92.27504244482174\n",
            "Epoch 212: train loss: 0.36020150780677795 train accuracy: 92.27504244482174\n",
            "Epoch 213: train loss: 0.35906124114990234 train accuracy: 92.27504244482174\n",
            "Epoch 214: train loss: 0.35792601108551025 train accuracy: 92.35993208828523\n",
            "Epoch 215: train loss: 0.35679593682289124 train accuracy: 92.35993208828523\n",
            "Epoch 216: train loss: 0.3556707799434662 train accuracy: 92.44482173174873\n",
            "Epoch 217: train loss: 0.35455071926116943 train accuracy: 92.44482173174873\n",
            "Epoch 218: train loss: 0.3534356653690338 train accuracy: 92.44482173174873\n",
            "Epoch 219: train loss: 0.3523257076740265 train accuracy: 92.44482173174873\n",
            "Epoch 220: train loss: 0.3512206971645355 train accuracy: 92.44482173174873\n",
            "Epoch 221: train loss: 0.35012078285217285 train accuracy: 92.44482173174873\n",
            "Epoch 222: train loss: 0.3490259647369385 train accuracy: 92.44482173174873\n",
            "Epoch 223: train loss: 0.34793615341186523 train accuracy: 92.44482173174873\n",
            "Epoch 224: train loss: 0.3468513488769531 train accuracy: 92.44482173174873\n",
            "Epoch 225: train loss: 0.3457714319229126 train accuracy: 92.44482173174873\n",
            "Epoch 226: train loss: 0.3446963131427765 train accuracy: 92.44482173174873\n",
            "Epoch 227: train loss: 0.3436260521411896 train accuracy: 92.44482173174873\n",
            "Epoch 228: train loss: 0.3425605893135071 train accuracy: 92.44482173174873\n",
            "Epoch 229: train loss: 0.341499924659729 train accuracy: 92.44482173174873\n",
            "Epoch 230: train loss: 0.34044408798217773 train accuracy: 92.44482173174873\n",
            "Epoch 231: train loss: 0.3393930196762085 train accuracy: 92.44482173174873\n",
            "Epoch 232: train loss: 0.3383469879627228 train accuracy: 92.44482173174873\n",
            "Epoch 233: train loss: 0.33730560541152954 train accuracy: 92.44482173174873\n",
            "Epoch 234: train loss: 0.3362690508365631 train accuracy: 92.44482173174873\n",
            "Epoch 235: train loss: 0.3352373242378235 train accuracy: 92.44482173174873\n",
            "Epoch 236: train loss: 0.33421051502227783 train accuracy: 92.44482173174873\n",
            "Epoch 237: train loss: 0.33318835496902466 train accuracy: 92.44482173174873\n",
            "Epoch 238: train loss: 0.33217087388038635 train accuracy: 92.44482173174873\n",
            "Epoch 239: train loss: 0.3311580419540405 train accuracy: 92.44482173174873\n",
            "Epoch 240: train loss: 0.33014997839927673 train accuracy: 92.52971137521223\n",
            "Epoch 241: train loss: 0.32914668321609497 train accuracy: 92.52971137521223\n",
            "Epoch 242: train loss: 0.3281480669975281 train accuracy: 92.52971137521223\n",
            "Epoch 243: train loss: 0.32715409994125366 train accuracy: 92.61460101867573\n",
            "Epoch 244: train loss: 0.32616472244262695 train accuracy: 92.61460101867573\n",
            "Epoch 245: train loss: 0.32517996430397034 train accuracy: 92.61460101867573\n",
            "Epoch 246: train loss: 0.3241996765136719 train accuracy: 92.61460101867573\n",
            "Epoch 247: train loss: 0.32322391867637634 train accuracy: 92.61460101867573\n",
            "Epoch 248: train loss: 0.32225272059440613 train accuracy: 92.78438030560271\n",
            "Epoch 249: train loss: 0.32128608226776123 train accuracy: 92.78438030560271\n",
            "Epoch 250: train loss: 0.32032397389411926 train accuracy: 92.78438030560271\n",
            "Epoch 251: train loss: 0.3193663954734802 train accuracy: 92.78438030560271\n",
            "Epoch 252: train loss: 0.31841328740119934 train accuracy: 92.86926994906621\n",
            "Epoch 253: train loss: 0.31746456027030945 train accuracy: 92.86926994906621\n",
            "Epoch 254: train loss: 0.3165203034877777 train accuracy: 93.0390492359932\n",
            "Epoch 255: train loss: 0.3155803978443146 train accuracy: 93.0390492359932\n",
            "Epoch 256: train loss: 0.3146449029445648 train accuracy: 93.0390492359932\n",
            "Epoch 257: train loss: 0.31371384859085083 train accuracy: 93.0390492359932\n",
            "Epoch 258: train loss: 0.3127870559692383 train accuracy: 93.0390492359932\n",
            "Epoch 259: train loss: 0.31186458468437195 train accuracy: 93.0390492359932\n",
            "Epoch 260: train loss: 0.31094640493392944 train accuracy: 93.0390492359932\n",
            "Epoch 261: train loss: 0.3100324273109436 train accuracy: 93.0390492359932\n",
            "Epoch 262: train loss: 0.3091227114200592 train accuracy: 93.0390492359932\n",
            "Epoch 263: train loss: 0.308217316865921 train accuracy: 93.0390492359932\n",
            "Epoch 264: train loss: 0.3073160946369171 train accuracy: 93.0390492359932\n",
            "Epoch 265: train loss: 0.30641910433769226 train accuracy: 93.0390492359932\n",
            "Epoch 266: train loss: 0.3055262267589569 train accuracy: 93.0390492359932\n",
            "Epoch 267: train loss: 0.3046375513076782 train accuracy: 93.1239388794567\n",
            "Epoch 268: train loss: 0.30375295877456665 train accuracy: 93.1239388794567\n",
            "Epoch 269: train loss: 0.30287250876426697 train accuracy: 93.2088285229202\n",
            "Epoch 270: train loss: 0.301996111869812 train accuracy: 93.2088285229202\n",
            "Epoch 271: train loss: 0.3011237382888794 train accuracy: 93.2937181663837\n",
            "Epoch 272: train loss: 0.3002554178237915 train accuracy: 93.2937181663837\n",
            "Epoch 273: train loss: 0.29939112067222595 train accuracy: 93.2937181663837\n",
            "Epoch 274: train loss: 0.2985308766365051 train accuracy: 93.2937181663837\n",
            "Epoch 275: train loss: 0.29767462611198425 train accuracy: 93.2937181663837\n",
            "Epoch 276: train loss: 0.29682230949401855 train accuracy: 93.2937181663837\n",
            "Epoch 277: train loss: 0.29597386717796326 train accuracy: 93.2937181663837\n",
            "Epoch 278: train loss: 0.29512935876846313 train accuracy: 93.2937181663837\n",
            "Epoch 279: train loss: 0.29428866505622864 train accuracy: 93.2937181663837\n",
            "Epoch 280: train loss: 0.2934519052505493 train accuracy: 93.2937181663837\n",
            "Epoch 281: train loss: 0.2926189601421356 train accuracy: 93.2937181663837\n",
            "Epoch 282: train loss: 0.2917897701263428 train accuracy: 93.2937181663837\n",
            "Epoch 283: train loss: 0.29096439480781555 train accuracy: 93.3786078098472\n",
            "Epoch 284: train loss: 0.29014289379119873 train accuracy: 93.3786078098472\n",
            "Epoch 285: train loss: 0.289325088262558 train accuracy: 93.3786078098472\n",
            "Epoch 286: train loss: 0.2885110378265381 train accuracy: 93.3786078098472\n",
            "Epoch 287: train loss: 0.28770071268081665 train accuracy: 93.4634974533107\n",
            "Epoch 288: train loss: 0.2868940830230713 train accuracy: 93.4634974533107\n",
            "Epoch 289: train loss: 0.2860911190509796 train accuracy: 93.4634974533107\n",
            "Epoch 290: train loss: 0.2852918803691864 train accuracy: 93.4634974533107\n",
            "Epoch 291: train loss: 0.2844963073730469 train accuracy: 93.4634974533107\n",
            "Epoch 292: train loss: 0.28370440006256104 train accuracy: 93.4634974533107\n",
            "Epoch 293: train loss: 0.28291603922843933 train accuracy: 93.4634974533107\n",
            "Epoch 294: train loss: 0.28213122487068176 train accuracy: 93.4634974533107\n",
            "Epoch 295: train loss: 0.2813499867916107 train accuracy: 93.54838709677419\n",
            "Epoch 296: train loss: 0.28057223558425903 train accuracy: 93.54838709677419\n",
            "Epoch 297: train loss: 0.27979809045791626 train accuracy: 93.63327674023769\n",
            "Epoch 298: train loss: 0.27902737259864807 train accuracy: 93.63327674023769\n",
            "Epoch 299: train loss: 0.27826017141342163 train accuracy: 93.63327674023769\n",
            "Epoch 300: train loss: 0.2774963974952698 train accuracy: 93.80305602716469\n",
            "Epoch 301: train loss: 0.2767360210418701 train accuracy: 93.80305602716469\n",
            "Epoch 302: train loss: 0.27597901225090027 train accuracy: 93.80305602716469\n",
            "Epoch 303: train loss: 0.2752254009246826 train accuracy: 93.80305602716469\n",
            "Epoch 304: train loss: 0.2744751274585724 train accuracy: 93.80305602716469\n",
            "Epoch 305: train loss: 0.27372825145721436 train accuracy: 93.80305602716469\n",
            "Epoch 306: train loss: 0.27298465371131897 train accuracy: 93.80305602716469\n",
            "Epoch 307: train loss: 0.2722444236278534 train accuracy: 93.80305602716469\n",
            "Epoch 308: train loss: 0.27150747179985046 train accuracy: 93.80305602716469\n",
            "Epoch 309: train loss: 0.2707737684249878 train accuracy: 93.80305602716469\n",
            "Epoch 310: train loss: 0.2700433135032654 train accuracy: 93.80305602716469\n",
            "Epoch 311: train loss: 0.26931607723236084 train accuracy: 93.80305602716469\n",
            "Epoch 312: train loss: 0.2685920000076294 train accuracy: 93.80305602716469\n",
            "Epoch 313: train loss: 0.26787111163139343 train accuracy: 93.80305602716469\n",
            "Epoch 314: train loss: 0.2671533524990082 train accuracy: 93.80305602716469\n",
            "Epoch 315: train loss: 0.2664387822151184 train accuracy: 93.80305602716469\n",
            "Epoch 316: train loss: 0.26572734117507935 train accuracy: 93.80305602716469\n",
            "Epoch 317: train loss: 0.2650189697742462 train accuracy: 93.88794567062818\n",
            "Epoch 318: train loss: 0.2643137276172638 train accuracy: 93.97283531409168\n",
            "Epoch 319: train loss: 0.2636116147041321 train accuracy: 93.97283531409168\n",
            "Epoch 320: train loss: 0.2629126012325287 train accuracy: 94.22750424448218\n",
            "Epoch 321: train loss: 0.26221662759780884 train accuracy: 94.22750424448218\n",
            "Epoch 322: train loss: 0.26152363419532776 train accuracy: 94.22750424448218\n",
            "Epoch 323: train loss: 0.2608337104320526 train accuracy: 94.22750424448218\n",
            "Epoch 324: train loss: 0.2601467967033386 train accuracy: 94.22750424448218\n",
            "Epoch 325: train loss: 0.25946277379989624 train accuracy: 94.22750424448218\n",
            "Epoch 326: train loss: 0.2587818205356598 train accuracy: 94.22750424448218\n",
            "Epoch 327: train loss: 0.25810369849205017 train accuracy: 94.22750424448218\n",
            "Epoch 328: train loss: 0.2574286460876465 train accuracy: 94.22750424448218\n",
            "Epoch 329: train loss: 0.256756454706192 train accuracy: 94.22750424448218\n",
            "Epoch 330: train loss: 0.25608715415000916 train accuracy: 94.22750424448218\n",
            "Epoch 331: train loss: 0.2554207444190979 train accuracy: 94.22750424448218\n",
            "Epoch 332: train loss: 0.2547571659088135 train accuracy: 94.31239388794567\n",
            "Epoch 333: train loss: 0.25409644842147827 train accuracy: 94.39728353140917\n",
            "Epoch 334: train loss: 0.2534386217594147 train accuracy: 94.39728353140917\n",
            "Epoch 335: train loss: 0.25278356671333313 train accuracy: 94.39728353140917\n",
            "Epoch 336: train loss: 0.2521314024925232 train accuracy: 94.39728353140917\n",
            "Epoch 337: train loss: 0.2514820396900177 train accuracy: 94.39728353140917\n",
            "Epoch 338: train loss: 0.2508353590965271 train accuracy: 94.39728353140917\n",
            "Epoch 339: train loss: 0.25019147992134094 train accuracy: 94.39728353140917\n",
            "Epoch 340: train loss: 0.24955029785633087 train accuracy: 94.39728353140917\n",
            "Epoch 341: train loss: 0.24891185760498047 train accuracy: 94.39728353140917\n",
            "Epoch 342: train loss: 0.24827609956264496 train accuracy: 94.39728353140917\n",
            "Epoch 343: train loss: 0.24764302372932434 train accuracy: 94.39728353140917\n",
            "Epoch 344: train loss: 0.24701261520385742 train accuracy: 94.39728353140917\n",
            "Epoch 345: train loss: 0.2463849037885666 train accuracy: 94.39728353140917\n",
            "Epoch 346: train loss: 0.2457597851753235 train accuracy: 94.39728353140917\n",
            "Epoch 347: train loss: 0.2451372891664505 train accuracy: 94.39728353140917\n",
            "Epoch 348: train loss: 0.24451743066310883 train accuracy: 94.39728353140917\n",
            "Epoch 349: train loss: 0.2439001053571701 train accuracy: 94.39728353140917\n",
            "Epoch 350: train loss: 0.24328535795211792 train accuracy: 94.39728353140917\n",
            "Epoch 351: train loss: 0.24267318844795227 train accuracy: 94.39728353140917\n",
            "Epoch 352: train loss: 0.2420634925365448 train accuracy: 94.39728353140917\n",
            "Epoch 353: train loss: 0.24145640432834625 train accuracy: 94.39728353140917\n",
            "Epoch 354: train loss: 0.24085180461406708 train accuracy: 94.39728353140917\n",
            "Epoch 355: train loss: 0.24024970829486847 train accuracy: 94.39728353140917\n",
            "Epoch 356: train loss: 0.23965008556842804 train accuracy: 94.39728353140917\n",
            "Epoch 357: train loss: 0.2390529215335846 train accuracy: 94.39728353140917\n",
            "Epoch 358: train loss: 0.23845824599266052 train accuracy: 94.48217317487267\n",
            "Epoch 359: train loss: 0.23786599934101105 train accuracy: 94.48217317487267\n",
            "Epoch 360: train loss: 0.23727619647979736 train accuracy: 94.48217317487267\n",
            "Epoch 361: train loss: 0.2366887629032135 train accuracy: 94.48217317487267\n",
            "Epoch 362: train loss: 0.23610377311706543 train accuracy: 94.48217317487267\n",
            "Epoch 363: train loss: 0.23552116751670837 train accuracy: 94.48217317487267\n",
            "Epoch 364: train loss: 0.23494097590446472 train accuracy: 94.48217317487267\n",
            "Epoch 365: train loss: 0.23436303436756134 train accuracy: 94.48217317487267\n",
            "Epoch 366: train loss: 0.2337874472141266 train accuracy: 94.56706281833617\n",
            "Epoch 367: train loss: 0.23321416974067688 train accuracy: 94.56706281833617\n",
            "Epoch 368: train loss: 0.2326432764530182 train accuracy: 94.56706281833617\n",
            "Epoch 369: train loss: 0.23207467794418335 train accuracy: 94.56706281833617\n",
            "Epoch 370: train loss: 0.23150834441184998 train accuracy: 94.56706281833617\n",
            "Epoch 371: train loss: 0.23094430565834045 train accuracy: 94.56706281833617\n",
            "Epoch 372: train loss: 0.23038256168365479 train accuracy: 94.56706281833617\n",
            "Epoch 373: train loss: 0.2298230081796646 train accuracy: 94.65195246179967\n",
            "Epoch 374: train loss: 0.2292657345533371 train accuracy: 94.65195246179967\n",
            "Epoch 375: train loss: 0.2287105917930603 train accuracy: 94.65195246179967\n",
            "Epoch 376: train loss: 0.22815768420696259 train accuracy: 94.82173174872665\n",
            "Epoch 377: train loss: 0.22760693728923798 train accuracy: 94.82173174872665\n",
            "Epoch 378: train loss: 0.22705836594104767 train accuracy: 94.82173174872665\n",
            "Epoch 379: train loss: 0.22651198506355286 train accuracy: 94.82173174872665\n",
            "Epoch 380: train loss: 0.22596774995326996 train accuracy: 94.82173174872665\n",
            "Epoch 381: train loss: 0.22542566061019897 train accuracy: 94.82173174872665\n",
            "Epoch 382: train loss: 0.22488568723201752 train accuracy: 94.82173174872665\n",
            "Epoch 383: train loss: 0.22434790432453156 train accuracy: 94.82173174872665\n",
            "Epoch 384: train loss: 0.22381219267845154 train accuracy: 94.82173174872665\n",
            "Epoch 385: train loss: 0.2232784926891327 train accuracy: 94.82173174872665\n",
            "Epoch 386: train loss: 0.22274692356586456 train accuracy: 94.82173174872665\n",
            "Epoch 387: train loss: 0.22221742570400238 train accuracy: 94.82173174872665\n",
            "Epoch 388: train loss: 0.22168999910354614 train accuracy: 94.82173174872665\n",
            "Epoch 389: train loss: 0.22116464376449585 train accuracy: 94.82173174872665\n",
            "Epoch 390: train loss: 0.22064125537872314 train accuracy: 94.82173174872665\n",
            "Epoch 391: train loss: 0.2201198786497116 train accuracy: 94.90662139219015\n",
            "Epoch 392: train loss: 0.21960052847862244 train accuracy: 94.90662139219015\n",
            "Epoch 393: train loss: 0.21908317506313324 train accuracy: 94.90662139219015\n",
            "Epoch 394: train loss: 0.21856780350208282 train accuracy: 94.90662139219015\n",
            "Epoch 395: train loss: 0.2180543839931488 train accuracy: 94.90662139219015\n",
            "Epoch 396: train loss: 0.21754294633865356 train accuracy: 94.99151103565364\n",
            "Epoch 397: train loss: 0.21703346073627472 train accuracy: 94.99151103565364\n",
            "Epoch 398: train loss: 0.21652592718601227 train accuracy: 94.99151103565364\n",
            "Epoch 399: train loss: 0.21602028608322144 train accuracy: 94.99151103565364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "or2ETNjIwmGN",
        "outputId": "c9138b4f-687d-49cc-9641-4321b7da236d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_epochs)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIerjb1RV5kf",
        "outputId": "c2d40abd-346b-4e03-f7f9-650b5cf12918"
      },
      "source": [
        "test_data_labels = fetch_20newsgroups(subset='test',categories=categories, shuffle=True)\n",
        "\n",
        "X_test_20newsgroups = data_20newsgroups_vectorizer.transform(test_data_labels.data) # we only use transform\n",
        "print(X_test_20newsgroups.shape)\n",
        "y_test_20newsgroups = test_data_labels.target\n",
        "\n",
        "num_test_samples = X_test_20newsgroups.toarray().shape[0]\n",
        "print('num test samples:',num_test_samples)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(785, 24614)\n",
            "num test samples: 785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnyBMrEy5m-k",
        "outputId": "63ad333a-8405-4050-8397-21c012eb7297"
      },
      "source": [
        "#set the neural net model to eval mode for testing\n",
        "model.eval()\n",
        "\n",
        "x_test = torch.FloatTensor(X_test_20newsgroups.toarray())\n",
        "y_test = torch.FloatTensor(y_test_20newsgroups).view(num_test_samples, 1)\n",
        "\n",
        "y_test_pred = model(x_test) #pass the test data to the model and get the predictions\n",
        "\n",
        "y_test_pred_cpu = y_test_pred.squeeze().detach().cpu().numpy()\n",
        "y_test_pred_cpu = np.multiply(y_test_pred_cpu>0.5,1)\n",
        "\n",
        "#print(y_test_20newsgroups)\n",
        "#print(y_test_pred_cpu)\n",
        "test_accuracy = 100.*np.sum(1-np.abs(y_test_pred_cpu-y_test_20newsgroups))/len(y_test_20newsgroups)\n",
        "\n",
        "print('Test set accuracy:',test_accuracy)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy: 85.85987261146497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr5LyMU7XPbU",
        "outputId": "6d401b48-2496-474e-854a-6d00db0112b7"
      },
      "source": [
        "newsgroup_test_corpus = ['The monitor in operation theatre has some snag', \n",
        "                         'What is the RAM size used in the PC?',\n",
        "                         'RBC and Haemoglobin are below normal.',\n",
        "                         'Renal disease is on the rise.',\n",
        "                         'Neurologists believe that this case is rare.']\n",
        "X_test_corpus = data_20newsgroups_vectorizer.transform(newsgroup_test_corpus) # we only use transform\n",
        "print(X_test_corpus.shape)\n",
        "\n",
        "num_test_samples = X_test_corpus.toarray().shape[0]\n",
        "print('num test samples:',num_test_samples)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "x_test = torch.FloatTensor(X_test_corpus.toarray())\n",
        "\n",
        "y_test_pred = model(x_test)\n",
        "\n",
        "y_test_pred_cpu = y_test_pred.squeeze().detach().cpu().numpy()\n",
        "y_test_pred_cpu = np.multiply(y_test_pred_cpu>0.5,1)\n",
        "\n",
        "print('predictions:')\n",
        "print(y_test_pred_cpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 24614)\n",
            "num test samples: 5\n",
            "predictions:\n",
            "[0 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large{\\textbf{Exercise}}$\n",
        "\n",
        "1. (a) Use unigram and bigram features in the code above and perform training using an appropriate feed forward network. Compute train set and test set accuracies. \n",
        "(b) Use unigram, bigram and trigram features in the code above and perform training using an appropriate feed forward network. Compute train set and test set accuracies. \n",
        "(c) Compare and contrast the train set and test set accuracies for parts (a) and (b) with those obtained in the code above. \n",
        "\n",
        "2. Use other classification techniques like SVM, decision tree, random forest, KNN for the classification problem above using (a) unigram, (b) unigram and bigram (c) unigram, bigram and trigram features. In each case, compute the train set and test set accuracies. Compare your results with those obtained from feed forward network above. \n",
        "\n",
        "2. (a). Select your $k$ favorite classes of newsgroups and perform multi-class classification training using the feed forward network (by modifying the output layer). Use unigram, bigram and trigram features. Compute train set and test set accuracies. \n",
        "   (b) Use other classification techniques like SVM, decision tree, random forests, KNN for the multi-class classification problem and compute the train set and test set accuracies. Compare your results with those obtained from feed forward network used in part (a). \n"
      ],
      "metadata": {
        "id": "AJk-_YPITHEw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL4C_69-Zn0d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}